{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558b3353-f9cf-45df-b91a-245ab5c64c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M2 GPU (mps).\n",
      "\n",
      "--- Training Model: ResNet50 ---\n",
      "\n",
      "[ResNet50] Stage 1/6: Loading and splitting data...\n",
      "Data loaded in 0.23s\n",
      "Found 55515 training images and 13878 validation images.\n",
      "Class indices: {'fake': 0, 'real': 1}\n",
      "\n",
      "[ResNet50] Stage 2/6: Creating model architecture...\n",
      "[ResNet50] Model created in 0.08s\n",
      "\n",
      "[ResNet50] Stage 3/6: Compiling and starting Phase 1 Training (Classifier Head)...\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Results: Train Loss: 0.5451 Acc: 0.7220 | Val Loss: 0.4750 Acc: 0.7718\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Results: Train Loss: 0.4698 Acc: 0.7757 | Val Loss: 0.4501 Acc: 0.7836\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Results: Train Loss: 0.4448 Acc: 0.7907 | Val Loss: 0.4275 Acc: 0.8026\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Results: Train Loss: 0.4243 Acc: 0.8006 | Val Loss: 0.4139 Acc: 0.8077\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Results: Train Loss: 0.4096 Acc: 0.8110 | Val Loss: 0.3993 Acc: 0.8179\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Results: Train Loss: 0.3925 Acc: 0.8197 | Val Loss: 0.3908 Acc: 0.8214\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Results: Train Loss: 0.3788 Acc: 0.8284 | Val Loss: 0.3781 Acc: 0.8307\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Results: Train Loss: 0.3618 Acc: 0.8388 | Val Loss: 0.3739 Acc: 0.8315\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Results: Train Loss: 0.3482 Acc: 0.8446 | Val Loss: 0.3636 Acc: 0.8375\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Results: Train Loss: 0.3363 Acc: 0.8503 | Val Loss: 0.3540 Acc: 0.8422\n",
      "[ResNet50] Phase 1 Training complete in 115.77 minutes.\n",
      "\n",
      "[ResNet50] Stage 4/6: Preparing model for Fine-Tuning...\n",
      "\n",
      "[ResNet50] Stage 5/6: Re-compiling model for Fine-Tuning...\n",
      "[ResNet50] Model re-compiled in 0.00s\n",
      "\n",
      "[ResNet50] Stage 6/6: Starting Phase 2 Training (Fine-Tuning)...\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(31233) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(31234) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Results: Train Loss: 0.1838 Acc: 0.9244 | Val Loss: 0.1004 Acc: 0.9604\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(31289) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(31290) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(31861) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(31863) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Results: Train Loss: 0.0593 Acc: 0.9785 | Val Loss: 0.0652 Acc: 0.9746\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(31903) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(31905) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(32425) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(32426) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Results: Train Loss: 0.0268 Acc: 0.9904 | Val Loss: 0.0548 Acc: 0.9780\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(32466) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(32467) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(33036) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33038) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Results: Train Loss: 0.0183 Acc: 0.9930 | Val Loss: 0.0517 Acc: 0.9813\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(33082) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33084) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(33606) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33607) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Results: Train Loss: 0.0144 Acc: 0.9943 | Val Loss: 0.0428 Acc: 0.9839\n",
      "[ResNet50] Phase 2 Training complete in 141.55 minutes.\n",
      "\n",
      "[ResNet50] Saving final model to deepfake_detector_ResNet50.pth...\n",
      "[ResNet50] Model saved in 0.41s\n",
      "\n",
      "--- ResNet50 Training Complete ---\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /Users/visheshbishnoi/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 20.5M/20.5M [00:00<00:00, 23.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model: EfficientNet_B0 ---\n",
      "\n",
      "[EfficientNet_B0] Stage 1/6: Loading and splitting data...\n",
      "Data loaded in 0.26s\n",
      "Found 55515 training images and 13878 validation images.\n",
      "Class indices: {'fake': 0, 'real': 1}\n",
      "\n",
      "[EfficientNet_B0] Stage 2/6: Creating model architecture...\n",
      "[EfficientNet_B0] Model created in 0.31s\n",
      "\n",
      "[EfficientNet_B0] Stage 3/6: Compiling and starting Phase 1 Training (Classifier Head)...\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(33646) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33648) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(33741) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33743) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Results: Train Loss: 0.5288 Acc: 0.7361 | Val Loss: 0.4460 Acc: 0.7946\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(33768) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33769) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(33909) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33912) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Results: Train Loss: 0.4761 Acc: 0.7716 | Val Loss: 0.4157 Acc: 0.8099\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(33933) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(33935) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(34117) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34119) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Results: Train Loss: 0.4506 Acc: 0.7881 | Val Loss: 0.3870 Acc: 0.8273\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(34170) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34177) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(34372) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34374) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Results: Train Loss: 0.4285 Acc: 0.8009 | Val Loss: 0.3719 Acc: 0.8323\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(34399) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34401) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(34580) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34581) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Results: Train Loss: 0.4072 Acc: 0.8106 | Val Loss: 0.3402 Acc: 0.8553\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(34636) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34638) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(34819) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34821) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Results: Train Loss: 0.3882 Acc: 0.8230 | Val Loss: 0.3253 Acc: 0.8571\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(34848) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(34849) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(35097) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35099) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Results: Train Loss: 0.3717 Acc: 0.8309 | Val Loss: 0.3136 Acc: 0.8639\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(35172) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35173) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(35350) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35357) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Results: Train Loss: 0.3619 Acc: 0.8372 | Val Loss: 0.2944 Acc: 0.8756\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(35434) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35436) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(35638) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35649) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Results: Train Loss: 0.3469 Acc: 0.8470 | Val Loss: 0.2896 Acc: 0.8755\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(35681) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35682) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(35889) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35890) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Results: Train Loss: 0.3368 Acc: 0.8507 | Val Loss: 0.2755 Acc: 0.8853\n",
      "[EfficientNet_B0] Phase 1 Training complete in 54.49 minutes.\n",
      "\n",
      "[EfficientNet_B0] Stage 4/6: Preparing model for Fine-Tuning...\n",
      "\n",
      "[EfficientNet_B0] Stage 5/6: Re-compiling model for Fine-Tuning...\n",
      "[EfficientNet_B0] Model re-compiled in 0.00s\n",
      "\n",
      "[EfficientNet_B0] Stage 6/6: Starting Phase 2 Training (Fine-Tuning)...\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(35936) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(35937) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(38026) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(38028) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Results: Train Loss: 0.2269 Acc: 0.9050 | Val Loss: 0.1211 Acc: 0.9554\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(38050) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(38051) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(41510) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(41512) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Results: Train Loss: 0.1331 Acc: 0.9481 | Val Loss: 0.0751 Acc: 0.9725\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(41535) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(41544) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(44690) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(44692) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Results: Train Loss: 0.0901 Acc: 0.9648 | Val Loss: 0.0569 Acc: 0.9785\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(44742) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(44743) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(48329) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(48331) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Results: Train Loss: 0.0686 Acc: 0.9738 | Val Loss: 0.0466 Acc: 0.9808\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                        | 0/1735 [00:00<?, ?it/s]python3.10(48355) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(48358) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Validating:   0%|                                       | 0/434 [00:00<?, ?it/s]python3.10(48919) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python3.10(48920) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Results: Train Loss: 0.0533 Acc: 0.9796 | Val Loss: 0.0375 Acc: 0.9847\n",
      "[EfficientNet_B0] Phase 2 Training complete in 313.68 minutes.\n",
      "\n",
      "[EfficientNet_B0] Saving final model to deepfake_detector_EfficientNet_B0.pth...\n",
      "[EfficientNet_B0] Model saved in 0.38s\n",
      "\n",
      "--- EfficientNet_B0 Training Complete ---\n",
      "\n",
      "--- All Model Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# --- PLEASE UPDATE THIS PATH ---\n",
    "# Point this to your new dataset folder that contains the 'real' and 'fake' subfolders.\n",
    "DATA_DIR = \"/Users/visheshbishnoi/Desktop/pd\"  \n",
    "# -----------------------------\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCHS = 10\n",
    "FINE_TUNE_EPOCHS = 5\n",
    "TOTAL_EPOCHS = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
    "VALIDATION_SPLIT = 0.2 # Use 20% of the data for validation\n",
    "NUM_WORKERS = 2 # Number of parallel workers for data loading\n",
    "\n",
    "# --- 2. Device Setup (Auto-detect M2 GPU) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M2 GPU (mps).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# --- 3. Data Transforms ---\n",
    "# PyTorch models pre-trained on ImageNet use these specific normalization values\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(IMG_HEIGHT, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# --- 4. Model Creation Functions ---\n",
    "def create_resnet50_model():\n",
    "    \"\"\"Creates a ResNet50 model for transfer learning.\"\"\"\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "    \n",
    "    # Freeze all base model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Replace the final classifier (fc) layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 1) # Output is 1 logit\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_efficientnet_model():\n",
    "    \"\"\"Creates an EfficientNet_B0 model for transfer learning.\"\"\"\n",
    "    # Note: torchvision has EfficientNet_B0, not V2 B0. \n",
    "    # This is the closest equivalent.\n",
    "    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze all base model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Replace the final classifier layer\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(num_ftrs, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(512, 1) # Output is 1 logit\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 5. Helper Functions for Training Loop ---\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Runs a single training epoch.\"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float().view(-1, 1)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Predict (apply sigmoid to logits, then threshold at 0.5)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            \n",
    "            # Backward pass + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset.indices)\n",
    "    epoch_acc = running_corrects.float() / len(loader.dataset.indices)\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Runs a single validation epoch.\"\"\"\n",
    "    model.eval() # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    pbar = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    # No gradients needed for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).float().view(-1, 1)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset.indices)\n",
    "    epoch_acc = running_corrects.float() / len(loader.dataset.indices)\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "# --- 6. Main Training Function ---\n",
    "def train_model_pytorch(model_name, model_creator_func):\n",
    "    \"\"\"\n",
    "    Trains and saves a single PyTorch model using a two-phase approach.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training Model: {model_name} ---\")\n",
    "    \n",
    "    # --- Data Loading ---\n",
    "    print(f\"\\n[{model_name}] Stage 1/6: Loading and splitting data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load the full dataset\n",
    "    full_dataset = datasets.ImageFolder(DATA_DIR)\n",
    "    \n",
    "    # Split the dataset\n",
    "    total_size = len(full_dataset)\n",
    "    val_size = int(total_size * VALIDATION_SPLIT)\n",
    "    train_size = total_size - val_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Assign the correct transforms to each split\n",
    "    train_dataset.dataset.transform = data_transforms['train']\n",
    "    val_dataset.dataset.transform = data_transforms['val']\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    print(f\"Data loaded in {time.time() - start_time:.2f}s\")\n",
    "    print(f\"Found {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "    print(f\"Class indices: {full_dataset.class_to_idx}\")\n",
    "\n",
    "    # --- Model Creation ---\n",
    "    print(f\"\\n[{model_name}] Stage 2/6: Creating model architecture...\")\n",
    "    start_time = time.time()\n",
    "    model = model_creator_func()\n",
    "    model = model.to(device) # Move model to M2 GPU\n",
    "    print(f\"[{model_name}] Model created in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # --- Phase 1: Training the Classifier Head ---\n",
    "    print(f\"\\n[{model_name}] Stage 3/6: Compiling and starting Phase 1 Training (Classifier Head)...\")\n",
    "    \n",
    "    # Use BCEWithLogitsLoss for numerical stability (takes raw logits)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Optimize only the parameters of the new classifier head\n",
    "    if model_name == \"ResNet50\":\n",
    "        optimizer = optim.Adam(model.fc.parameters(), lr=0.0001)\n",
    "    else: # EfficientNet\n",
    "        optimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(INITIAL_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{TOTAL_EPOCHS}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} Results: Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"[{model_name}] Phase 1 Training complete in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "    # --- Phase 2: Fine-Tuning ---\n",
    "    print(f\"\\n[{model_name}] Stage 4/6: Preparing model for Fine-Tuning...\")\n",
    "    \n",
    "    # Unfreeze all model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Re-compile with a very low learning rate for fine-tuning\n",
    "    print(f\"\\n[{model_name}] Stage 5/6: Re-compiling model for Fine-Tuning...\")\n",
    "    start_time = time.time()\n",
    "    # Create a new optimizer for ALL parameters with a low LR\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "    print(f\"[{model_name}] Model re-compiled in {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n[{model_name}] Stage 6/6: Starting Phase 2 Training (Fine-Tuning)...\")\n",
    "    start_time = time.time()\n",
    "    # Continue training\n",
    "    for epoch in range(INITIAL_EPOCHS, TOTAL_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{TOTAL_EPOCHS}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} Results: Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"[{model_name}] Phase 2 Training complete in {(time.time() - start_time)/60:.2f} minutes.\")\n",
    "    \n",
    "    # --- Save Model ---\n",
    "    model_filename = f'deepfake_detector_{model_name}.pth' # .pth is the standard PyTorch extension\n",
    "    print(f\"\\n[{model_name}] Saving final model to {model_filename}...\")\n",
    "    start_time = time.time()\n",
    "    # Save the model's learned parameters (state dictionary)\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    print(f\"[{model_name}] Model saved in {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\n--- {model_name} Training Complete ---\")\n",
    "\n",
    "# --- 7. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # Check if data directory exists\n",
    "    if not os.path.exists(DATA_DIR) or DATA_DIR == \"./path_to_your_new_dataset_folder\":\n",
    "        print(f\"Error: Data directory not found at '{DATA_DIR}'\")\n",
    "        print(\"Please update the 'DATA_DIR' variable at the top of the script.\")\n",
    "    else:\n",
    "        # Train models sequentially\n",
    "        \n",
    "        # 1. Train ResNet50\n",
    "        model_resnet = create_resnet50_model()\n",
    "        train_model_pytorch(\"ResNet50\", lambda: model_resnet)\n",
    "        \n",
    "        # 2. Train EfficientNet_B0\n",
    "        model_efficientnet = create_efficientnet_model()\n",
    "        train_model_pytorch(\"EfficientNet_B0\", lambda: model_efficientnet)\n",
    "        \n",
    "        print(\"\\n--- All Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a01234-d5b7-4366-9097-56e64af6de09",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62414c0d-8c0e-4e3a-a50a-872a8a7f2337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M2 GPU (mps).\n",
      "\n",
      "--- Loading Ensemble Models ---\n",
      "Loading ResNet50 from deepfake_detector_ResNet50.pth...\n",
      "Loading EfficientNet_B0 from deepfake_detector_EfficientNet_B0.pth...\n",
      "\n",
      "Models loaded and set to eval() mode.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/celeb_fake_1891.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/data/fake/celeb_fake_1891.mp4\n",
      "Total frames to process: 331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 331/331 [00:10<00:00, 32.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 10.23 seconds\n",
      "Total frames processed: 331\n",
      "Frames with faces detected: 296\n",
      "---------------------------------\n",
      "Frames classified as REAL: 2\n",
      "Frames classified as FAKE: 294\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY FAKE (99.32% of face-frames were fake)\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1351.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1351.mp4\n",
      "Total frames to process: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 299/299 [00:10<00:00, 28.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 10.51 seconds\n",
      "Total frames processed: 299\n",
      "Frames with faces detected: 219\n",
      "---------------------------------\n",
      "Frames classified as REAL: 219\n",
      "Frames classified as FAKE: 0\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL (100.00% of face-frames were real)\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1111.mp4.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file was not found at '/Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1111.mp4.mp4'. Please check the path.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1111.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1111.mp4\n",
      "Total frames to process: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 299/299 [00:12<00:00, 24.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 12.07 seconds\n",
      "Total frames processed: 299\n",
      "Frames with faces detected: 299\n",
      "---------------------------------\n",
      "Frames classified as REAL: 297\n",
      "Frames classified as FAKE: 2\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL (99.33% of face-frames were real)\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_0046.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_0046.mp4\n",
      "Total frames to process: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 299/299 [00:08<00:00, 37.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 8.01 seconds\n",
      "Total frames processed: 299\n",
      "Frames with faces detected: 6\n",
      "---------------------------------\n",
      "Frames classified as REAL: 4\n",
      "Frames classified as FAKE: 2\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL (66.67% of face-frames were real)\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/real/dfdc_real_0105.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/data/real/dfdc_real_0105.mp4\n",
      "Total frames to process: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 299/299 [00:08<00:00, 35.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 8.34 seconds\n",
      "Total frames processed: 299\n",
      "Frames with faces detected: 93\n",
      "---------------------------------\n",
      "Frames classified as REAL: 92\n",
      "Frames classified as FAKE: 1\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL (98.92% of face-frames were real)\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/real/celeb_real_0501.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/data/real/celeb_real_0501.mp4\n",
      "Total frames to process: 376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 376/376 [00:10<00:00, 34.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 10.80 seconds\n",
      "Total frames processed: 376\n",
      "Frames with faces detected: 376\n",
      "---------------------------------\n",
      "Frames classified as REAL: 376\n",
      "Frames classified as FAKE: 0\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL (100.00% of face-frames were real)\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Update these paths to your saved PyTorch models\n",
    "MODEL_PATHS = {\n",
    "    \"ResNet50\": 'deepfake_detector_ResNet50.pth',\n",
    "    \"EfficientNet_B0\": 'deepfake_detector_EfficientNet_B0.pth'\n",
    "}\n",
    "HAAR_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "# --- 2. Device Setup (Auto-detect M2 GPU) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M2 GPU (mps).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# --- 3. Data Transform (for prediction) ---\n",
    "# This MUST be the same normalization as your validation data\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 4. Model Creation Functions ---\n",
    "# We need to define the model architectures to load the saved weights (state_dict)\n",
    "def create_resnet50_model():\n",
    "    model = models.resnet50(weights=None) # Load architecture, not weights\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_efficientnet_model():\n",
    "    model = models.efficientnet_b0(weights=None) # Load architecture, not weights\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(num_ftrs, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(512, 1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 5. Load Models ---\n",
    "def load_all_models(paths):\n",
    "    \"\"\"Loads all trained PyTorch models.\"\"\"\n",
    "    models_ensemble = {}\n",
    "    print(\"\\n--- Loading Ensemble Models ---\")\n",
    "\n",
    "    # 1. Load ResNet50\n",
    "    if os.path.exists(paths[\"ResNet50\"]):\n",
    "        print(f\"Loading ResNet50 from {paths['ResNet50']}...\")\n",
    "        model_resnet = create_resnet50_model()\n",
    "        model_resnet.load_state_dict(torch.load(paths[\"ResNet50\"], map_location=device))\n",
    "        model_resnet = model_resnet.to(device)\n",
    "        model_resnet.eval() # Set model to evaluation mode (CRITICAL)\n",
    "        models_ensemble[\"ResNet50\"] = model_resnet\n",
    "    else:\n",
    "        print(f\"Warning: Model file not found at '{paths['ResNet50']}'.\")\n",
    "\n",
    "    # 2. Load EfficientNet_B0\n",
    "    if os.path.exists(paths[\"EfficientNet_B0\"]):\n",
    "        print(f\"Loading EfficientNet_B0 from {paths['EfficientNet_B0']}...\")\n",
    "        model_efficientnet = create_efficientnet_model()\n",
    "        model_efficientnet.load_state_dict(torch.load(paths[\"EfficientNet_B0\"], map_location=device))\n",
    "        model_efficientnet = model_efficientnet.to(device)\n",
    "        model_efficientnet.eval() # Set model to evaluation mode\n",
    "        models_ensemble[\"EfficientNet_B0\"] = model_efficientnet\n",
    "    else:\n",
    "        print(f\"Warning: Model file not found at '{paths['EfficientNet_B0']}'.\")\n",
    "\n",
    "    if not models_ensemble:\n",
    "        print(\"\\nERROR: No models could be loaded. Please check the MODEL_PATHS.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\nModels loaded and set to eval() mode.\")\n",
    "    return models_ensemble\n",
    "\n",
    "# --- 6. Video Analysis Function ---\n",
    "def analyze_video(video_path, models_ensemble, face_cascade):\n",
    "    \"\"\"\n",
    "    Opens a video file, analyzes it frame by frame, and provides a final verdict.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file at '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    # Get video properties for the progress bar\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    except:\n",
    "        total_frames = 0\n",
    "        \n",
    "    print(f\"\\nAnalyzing video file: {video_path}\")\n",
    "    print(f\"Total frames to process: {total_frames}\")\n",
    "    \n",
    "    real_votes = 0\n",
    "    fake_votes = 0\n",
    "    faces_found_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # End of video\n",
    "        \n",
    "        # --- Face Detection ---\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "        \n",
    "        # We'll just analyze the first (and likely only) face found in the frame\n",
    "        if len(faces) > 0:\n",
    "            faces_found_count += 1\n",
    "            (x, y, w, h) = faces[0]\n",
    "            \n",
    "            # Crop the face\n",
    "            face_crop_bgr = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # --- Preprocessing for PyTorch ---\n",
    "            face_crop_rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(face_crop_rgb)\n",
    "            input_tensor = data_transform(pil_image)\n",
    "            input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # --- Get Ensemble Prediction ---\n",
    "            all_predictions = []\n",
    "            with torch.no_grad(): # Disable gradient calculation\n",
    "                for model in models_ensemble.values():\n",
    "                    logit = model(input_batch)\n",
    "                    prob = torch.sigmoid(logit).item()\n",
    "                    all_predictions.append(prob)\n",
    "            \n",
    "            # Average the probabilities\n",
    "            average_prediction = sum(all_predictions) / len(all_predictions)\n",
    "            \n",
    "            # Tally votes\n",
    "            if average_prediction < 0.5:\n",
    "                fake_votes += 1\n",
    "            else:\n",
    "                real_votes += 1\n",
    "        \n",
    "        pbar.update(1) # Update progress bar\n",
    "        \n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    \n",
    "    # --- Print Final Report ---\n",
    "    print(\"\\n--- Video Analysis Complete ---\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Total frames processed: {total_frames}\")\n",
    "    print(f\"Frames with faces detected: {faces_found_count}\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Frames classified as REAL: {real_votes}\")\n",
    "    print(f\"Frames classified as FAKE: {fake_votes}\")\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "    if faces_found_count == 0:\n",
    "        print(\"Final Verdict: UNKNOWN (No faces were detected in the video)\")\n",
    "    else:\n",
    "        fake_percentage = (fake_votes / faces_found_count) * 100\n",
    "        real_percentage = (real_votes / faces_found_count) * 100\n",
    "        \n",
    "        if fake_percentage > 50:\n",
    "            print(f\"Final Verdict: LIKELY FAKE ({fake_percentage:.2f}% of face-frames were fake)\")\n",
    "        else:\n",
    "            print(f\"Final Verdict: LIKELY REAL ({real_percentage:.2f}% of face-frames were real)\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "# --- 7. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    models = load_all_models(MODEL_PATHS)\n",
    "    face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "    \n",
    "    if models: # Only proceed if models were loaded\n",
    "        while True:\n",
    "            video_path = input(\"\\nEnter the path to the video you want to analyze (or 'exit' to quit): \").strip()\n",
    "            \n",
    "            if video_path.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"Error: The file was not found at '{video_path}'. Please check the path.\")\n",
    "                continue\n",
    "                \n",
    "            analyze_video(video_path, models, face_cascade)\n",
    "    else:\n",
    "        print(\"Exiting. No models were loaded to perform analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515aadf-6be3-4f3b-b46b-af5ad6f70fc0",
   "metadata": {},
   "source": [
    "video-ccn+lstm preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57ccf72-bdf1-40e2-9788-94603ab55e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Video Preprocessing for Temporal Model ---\n",
      "Found 2048 total videos.\n",
      "Splitting data: 1432 train, 308 val, 308 test videos.\n",
      "\n",
      "Processing 'train' set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train videos: 100%|██████████████| 1432/1432 [18:25<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 'val' set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val videos: 100%|██████████████████| 308/308 [03:40<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 'test' set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test videos: 100%|█████████████████| 308/308 [03:22<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Complete ---\n",
      "Frame sequences saved to: /Users/visheshbishnoi/Desktop/data1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import shutil # <--- 1. ADD THIS IMPORT\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# --- PLEASE UPDATE THESE PATHS ---\n",
    "# Path to your video dataset (containing 'real' and 'fake' subfolders)\n",
    "INPUT_VIDEO_DIR = \"/Users/visheshbishnoi/Desktop/data\"\n",
    "# Path to the output directory where processed frames will be saved\n",
    "OUTPUT_FRAME_DIR = \"/Users/visheshbishnoi/Desktop/data1\"\n",
    "# --------------------------------\n",
    "\n",
    "# --- Advanced Settings ---\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "# How many frames to extract from each video. This will be the sequence length.\n",
    "NUM_FRAMES = 30 \n",
    "# Split ratios for train, validation, and test sets\n",
    "TEST_SPLIT = 0.15\n",
    "VAL_SPLIT = 0.15\n",
    "\n",
    "HAAR_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "def get_frame_indices(total_frames, num_frames_to_extract):\n",
    "    \"\"\"Calculates evenly spaced frame indices to extract.\"\"\"\n",
    "    if total_frames < num_frames_to_extract:\n",
    "        # If video is too short, duplicate the last frame\n",
    "        indices = np.arange(total_frames).tolist()\n",
    "        indices.extend([total_frames - 1] * (num_frames_to_extract - total_frames))\n",
    "    else:\n",
    "        # Get evenly spaced indices\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames_to_extract, dtype=int)\n",
    "    return indices\n",
    "\n",
    "def process_and_save_frame(frame, output_path):\n",
    "    \"\"\"Detects face, crops, resizes, and saves the frame.\"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        (x, y, w, h) = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "        resized_face = cv2.resize(face_crop, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        cv2.imwrite(output_path, resized_face)\n",
    "        return True\n",
    "    return False # No face found\n",
    "\n",
    "# --- 3. Main Processing Logic ---\n",
    "def process_videos():\n",
    "    print(\"--- Starting Video Preprocessing for Temporal Model ---\")\n",
    "    \n",
    "    all_video_files = [] # Will store (label, path) tuples\n",
    "    for label in ['real', 'fake']:\n",
    "        video_dir = os.path.join(INPUT_VIDEO_DIR, label)\n",
    "        if not os.path.exists(video_dir):\n",
    "            print(f\"Warning: Directory not found, skipping: {video_dir}\")\n",
    "            continue\n",
    "            \n",
    "        for ext in ('*.mp4', '*.mov', '*.avi'):\n",
    "            all_video_files.extend([(label, p) for p in glob.glob(os.path.join(video_dir, ext))])\n",
    "    \n",
    "    if not all_video_files:\n",
    "        print(f\"Error: No video files found in {INPUT_VIDEO_DIR}.\")\n",
    "        print(\"Please check your INPUT_VIDEO_DIR path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(all_video_files)} total videos.\")\n",
    "    \n",
    "    # --- Split videos into train, val, and test SETS ---\n",
    "    # We split by video, not by frame, to prevent data leakage\n",
    "    train_val_files, test_files = train_test_split(all_video_files, test_size=TEST_SPLIT, random_state=42)\n",
    "    val_ratio = VAL_SPLIT / (1.0 - TEST_SPLIT) # Adjust val split based on remaining data\n",
    "    train_files, val_files = train_test_split(train_val_files, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    datasets = {\n",
    "        'train': train_files,\n",
    "        'val': val_files,\n",
    "        'test': test_files\n",
    "    }\n",
    "\n",
    "    print(f\"Splitting data: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test videos.\")\n",
    "\n",
    "    # --- Process and Save Frames ---\n",
    "    for split_name, video_list in datasets.items():\n",
    "        print(f\"\\nProcessing '{split_name}' set...\")\n",
    "        split_output_dir = os.path.join(OUTPUT_FRAME_DIR, split_name)\n",
    "        \n",
    "        for label, video_path in tqdm(video_list, desc=f\"Processing {split_name} videos\"):\n",
    "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "            \n",
    "            # Create the final output directory for this video's frames\n",
    "            # e.g., .../temporal_dataset/train/real/video_001/\n",
    "            video_frame_dir = os.path.join(split_output_dir, label, video_name)\n",
    "            if os.path.exists(video_frame_dir):\n",
    "                continue # Skip if already processed\n",
    "            os.makedirs(video_frame_dir, exist_ok=True)\n",
    "            \n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Warning: Could not open {video_path}\")\n",
    "                continue\n",
    "            \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if total_frames == 0:\n",
    "                continue\n",
    "\n",
    "            frame_indices = get_frame_indices(total_frames, NUM_FRAMES)\n",
    "            \n",
    "            frames_saved = 0\n",
    "            for i, frame_num in enumerate(frame_indices):\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                \n",
    "                output_filename = f\"{i:03d}.jpg\" # e.g., 001.jpg, 002.jpg\n",
    "                output_path = os.path.join(video_frame_dir, output_filename)\n",
    "                \n",
    "                if process_and_save_frame(frame, output_path):\n",
    "                    frames_saved += 1\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # If we couldn't save all frames (e.g., no face detected),\n",
    "            # remove the folder to avoid partial data\n",
    "            if frames_saved < NUM_FRAMES:\n",
    "                if os.path.exists(video_frame_dir):\n",
    "                    # In a real scenario, you might want to handle this differently\n",
    "                    # For now, we remove it to ensure clean sequences\n",
    "                    shutil.rmtree(video_frame_dir) # <--- 2. CHANGE THIS LINE\n",
    "\n",
    "    print(\"\\n--- Preprocessing Complete ---\")\n",
    "    print(f\"Frame sequences saved to: {OUTPUT_FRAME_DIR}\")\n",
    "\n",
    "# --- 4. Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(HAAR_CASCADE_PATH):\n",
    "        print(f\"Error: Could not find Haar Cascade file at {HAAR_CASCADE_PATH}\")\n",
    "    else:\n",
    "        process_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b01efe-4619-48ca-9437-a2901b918fd6",
   "metadata": {},
   "source": [
    "lstm model training:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d27e679-d872-49f8-845d-6e548802ba6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M2 GPU (mps).\n",
      "Loading datasets...\n",
      "Found 1071 training sequences and 228 validation sequences.\n",
      "Initializing temporal (CNN+LSTM) model...\n",
      "Loading pretrained weights from: ./deepfake_detector_EfficientNet_B0.pth\n",
      "--- Starting Temporal Model Training ---\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.4679 Acc: 0.8263 | Val Loss: 0.2176 Acc: 0.9254\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2645 Acc: 0.8992 | Val Loss: 0.1612 Acc: 0.9474\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2648 Acc: 0.8936 | Val Loss: 0.1620 Acc: 0.9474\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2351 Acc: 0.9057 | Val Loss: 0.1363 Acc: 0.9561\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2147 Acc: 0.9122 | Val Loss: 0.1554 Acc: 0.9430\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1479 Acc: 0.9458 | Val Loss: 0.2530 Acc: 0.9123\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2033 Acc: 0.9197 | Val Loss: 0.1091 Acc: 0.9561\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2115 Acc: 0.9122 | Val Loss: 0.1348 Acc: 0.9518\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1990 Acc: 0.9225 | Val Loss: 0.1411 Acc: 0.9605\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1643 Acc: 0.9356 | Val Loss: 0.1473 Acc: 0.9649\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.2065 Acc: 0.9197 | Val Loss: 0.1223 Acc: 0.9518\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1294 Acc: 0.9486 | Val Loss: 0.1759 Acc: 0.9342\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1406 Acc: 0.9496 | Val Loss: 0.1554 Acc: 0.9386\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1437 Acc: 0.9505 | Val Loss: 0.1262 Acc: 0.9518\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Results: Train Loss: 0.1435 Acc: 0.9477 | Val Loss: 0.1760 Acc: 0.9430\n",
      "\n",
      "Training complete. Saving final temporal model to deepfake_temporal_detector.pth...\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# --- PLEASE UPDATE THESE PATHS ---\n",
    "# Path to your preprocessed temporal dataset (output of preprocess_videos.py)\n",
    "TEMPORAL_DATA_DIR = \"/Users/visheshbishnoi/Desktop/data1\" \n",
    "# Path to your PRE-TRAINED EfficientNet_B0 model\n",
    "PRETRAINED_MODEL_PATH = \"./deepfake_detector_EfficientNet_B0.pth\"\n",
    "# --------------------------------\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 8\n",
    "NUM_FRAMES = 30 \n",
    "NUM_WORKERS = 0 # Set to 0 to fix multiprocessing error on macOS/Windows\n",
    "EPOCHS = 15\n",
    "\n",
    "# --- 2. Device Setup (Auto-detect M2 GPU) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M2 GPU (mps).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# --- 3. Custom PyTorch Dataset ---\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, data_dir, num_frames, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.samples = [] \n",
    "        \n",
    "        for label, class_name in enumerate(['fake', 'real']):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Warning: Directory not found, skipping: {class_dir}\")\n",
    "                continue\n",
    "                \n",
    "            for video_folder in os.listdir(class_dir):\n",
    "                video_folder_path = os.path.join(class_dir, video_folder)\n",
    "                if os.path.isdir(video_folder_path):\n",
    "                    self.samples.append((video_folder_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_folder_path, label = self.samples[idx]\n",
    "        \n",
    "        frames = sorted(\n",
    "            glob.glob(os.path.join(video_folder_path, '*.jpg')),\n",
    "            key=lambda x: int(os.path.basename(x).split('.')[0])\n",
    "        )\n",
    "        \n",
    "        if len(frames) != self.num_frames:\n",
    "            if len(frames) == 0:\n",
    "                black_img = Image.new('RGB', (IMG_WIDTH, IMG_HEIGHT))\n",
    "                sequence = [black_img] * self.num_frames\n",
    "            else:\n",
    "                sequence = [Image.open(f) for f in frames]\n",
    "                sequence.extend([sequence[-1]] * (self.num_frames - len(sequence)))\n",
    "        else:\n",
    "            sequence = [Image.open(f) for f in frames[:self.num_frames]]\n",
    "\n",
    "        if self.transform:\n",
    "            sequence = [self.transform(img) for img in sequence]\n",
    "            \n",
    "        sequence_tensor = torch.stack(sequence)\n",
    "        \n",
    "        return sequence_tensor, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# --- 4. Data Transforms (Must match previous training) ---\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 5. Model Architecture (CNN+LSTM) ---\n",
    "class CNN_LSTM_Model(nn.Module):\n",
    "    def __init__(self, pretrained_model_path):\n",
    "        super(CNN_LSTM_Model, self).__init__()\n",
    "        \n",
    "        # --- 1. Load the CNN Feature Extractor ---\n",
    "        # Load the base EfficientNet_B0 architecture\n",
    "        self.cnn_base = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        # Get the original feature size and replace the classifier\n",
    "        num_ftrs = self.cnn_base.classifier[1].in_features # 1280 for B0\n",
    "        self.cnn_base.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Load the weights we trained in the *previous* step\n",
    "        print(f\"Loading pretrained weights from: {pretrained_model_path}\")\n",
    "        self.cnn_base.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
    "        \n",
    "        # We only want the features, so we remove the final classifier\n",
    "        # For EfficientNet, the features are in the 'avgpool' layer\n",
    "        self.cnn_feature_extractor = nn.Sequential(*list(self.cnn_base.children())[:-1])\n",
    "        \n",
    "        # --- 2. Freeze the CNN ---\n",
    "        for param in self.cnn_feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # --- 3. The LSTM (Temporal) part ---\n",
    "        self.lstm_hidden_size = 512\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_ftrs, # 1280 for EfficientNet_B0\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True \n",
    "        )\n",
    "        \n",
    "        # --- 4. The Final Classifier ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.lstm_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1) # Final output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_frames, channels, height, width)\n",
    "        batch_size, num_frames, c, h, w = x.shape\n",
    "        \n",
    "        x = x.view(batch_size * num_frames, c, h, w)\n",
    "        \n",
    "        # Get features from CNN\n",
    "        # output: (batch_size * num_frames, 1280, 1, 1)\n",
    "        cnn_features = self.cnn_feature_extractor(x)\n",
    "        \n",
    "        # Reshape to (batch_size * num_frames, 1280)\n",
    "        cnn_features = cnn_features.view(batch_size * num_frames, -1)\n",
    "        \n",
    "        # Reshape back to sequence: (batch_size, num_frames, 1280)\n",
    "        sequence_features = cnn_features.view(batch_size, num_frames, -1)\n",
    "        \n",
    "        # Pass sequence to LSTM\n",
    "        lstm_out, _ = self.lstm(sequence_features)\n",
    "        \n",
    "        # We only care about the output of the *last* frame in the sequence\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass to the final classifier\n",
    "        logit = self.classifier(last_time_step_out)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "# --- 6. Main Training Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Check for dataset and pretrained model ---\n",
    "    if not os.path.exists(TEMPORAL_DATA_DIR):\n",
    "        print(f\"Error: Temporal data directory not found at '{TEMPORAL_DATA_DIR}'\")\n",
    "        print(\"Please run 'preprocess_videos.py' first.\")\n",
    "        exit()\n",
    "    if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "        print(f\"Error: Pretrained EfficientNet model not found at '{PRETRAINED_MODEL_PATH}'\")\n",
    "        print(\"Please ensure the .pth file is in this directory.\")\n",
    "        exit()\n",
    "\n",
    "    # --- Create Datasets and DataLoaders ---\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = VideoFrameDataset(\n",
    "        data_dir=os.path.join(TEMPORAL_DATA_DIR, 'train'),\n",
    "        num_frames=NUM_FRAMES,\n",
    "        transform=data_transform\n",
    "    )\n",
    "    val_dataset = VideoFrameDataset(\n",
    "        data_dir=os.path.join(TEMPORAL_DATA_DIR, 'val'),\n",
    "        num_frames=NUM_FRAMES,\n",
    "        transform=data_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    print(f\"Found {len(train_dataset)} training sequences and {len(val_dataset)} validation sequences.\")\n",
    "\n",
    "    # --- Initialize Model, Loss, Optimizer ---\n",
    "    print(\"Initializing temporal (CNN+LSTM) model...\")\n",
    "    model = CNN_LSTM_Model(PRETRAINED_MODEL_PATH).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # We only optimize the parameters of the LSTM and the new classifier\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.lstm.parameters()) + list(model.classifier.parameters()),\n",
    "        lr=0.0001\n",
    "    )\n",
    "    \n",
    "    print(\"--- Starting Temporal Model Training ---\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1, 1) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_dataset)\n",
    "        epoch_train_acc = running_corrects.float() / len(train_dataset)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).view(-1, 1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_val_loss = running_loss / len(val_dataset)\n",
    "        epoch_val_acc = running_corrects.float() / len(val_dataset)\n",
    "        \n",
    "        print(f\"Epoch Results: Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
    "        \n",
    "    # --- Save Final Model ---\n",
    "    model_filename = 'deepfake_temporal_detector.pth'\n",
    "    print(f\"\\nTraining complete. Saving final temporal model to {model_filename}...\")\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65093de-460c-470a-98fe-cd22a689ee7c",
   "metadata": {},
   "source": [
    "temporal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66c9973-b11f-4883-9cef-8e580bfff42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M2 GPU (mps).\n",
      "Loading temporal (EfficientNet+LSTM) model...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1111.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video: /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_1111.mp4\n",
      "\n",
      "--- Verdict: FAKE ---\n",
      "Confidence: 75.51%\n",
      "Analysis took 2.65 seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_0046.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video: /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_0046.mp4\n",
      "\n",
      "--- Verdict: FAKE ---\n",
      "Confidence: 99.34%\n",
      "Analysis took 5.02 seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Path to your FINAL saved temporal model (the EfficientNet+LSTM one)\n",
    "SAVED_MODEL_PATH = \"./deepfake_temporal_detector.pth\"\n",
    "# --------------------------------\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_FRAMES = 30 # Must match the training sequence length\n",
    "\n",
    "HAAR_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "\n",
    "# --- 2. Device Setup (Auto-detect M2 GPU) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M2 GPU (mps).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# --- 3. Data Transform (Must match training) ---\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 4. Model Architecture (Must match training) ---\n",
    "# We must redefine the model architecture to load the weights\n",
    "class CNN_LSTM_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LSTM_Model, self).__init__()\n",
    "        \n",
    "        # --- 1. Load the CNN Feature Extractor ---\n",
    "        self.cnn_base = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        num_ftrs = self.cnn_base.classifier[1].in_features\n",
    "        self.cnn_base.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # We only want the features\n",
    "        self.cnn_feature_extractor = nn.Sequential(*list(self.cnn_base.children())[:-1])\n",
    "            \n",
    "        # --- 2. The LSTM (Temporal) part ---\n",
    "        self.lstm_hidden_size = 512\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_ftrs, # 1280 for EfficientNet_B0\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True \n",
    "        )\n",
    "        \n",
    "        # --- 3. The Final Classifier ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.lstm_hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1) # Final output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, c, h, w = x.shape\n",
    "        x = x.view(batch_size * num_frames, c, h, w)\n",
    "        cnn_features = self.cnn_feature_extractor(x)\n",
    "        cnn_features = cnn_features.view(batch_size * num_frames, -1)\n",
    "        sequence_features = cnn_features.view(batch_size, num_frames, -1)\n",
    "        lstm_out, _ = self.lstm(sequence_features)\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        logit = self.classifier(last_time_step_out)\n",
    "        return logit\n",
    "\n",
    "# --- 5. Frame Extraction & Preprocessing Logic ---\n",
    "def get_frame_indices(total_frames, num_frames_to_extract):\n",
    "    \"\"\"Calculates evenly spaced frame indices to extract.\"\"\"\n",
    "    if total_frames < num_frames_to_extract:\n",
    "        indices = np.arange(total_frames).tolist()\n",
    "        indices.extend([total_frames - 1] * (num_frames_to_extract - total_frames))\n",
    "    else:\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames_to_extract, dtype=int)\n",
    "    return indices\n",
    "\n",
    "def process_video_for_prediction(video_path):\n",
    "    \"\"\"\n",
    "    Extracts and processes 30 frames from a video file,\n",
    "    returning a tensor ready for the model.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return None\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames == 0:\n",
    "        print(\"Error: Video file is empty or corrupt.\")\n",
    "        return None\n",
    "\n",
    "    frame_indices = get_frame_indices(total_frames, NUM_FRAMES)\n",
    "    \n",
    "    frame_sequence = []\n",
    "    frames_processed_count = 0\n",
    "    \n",
    "    for frame_num in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # --- Face Detection and Cropping ---\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "        \n",
    "        face_crop = None\n",
    "        if len(faces) > 0:\n",
    "            # Get the largest face\n",
    "            (x, y, w, h) = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]\n",
    "            face_crop = frame[y:y+h, x:x+w]\n",
    "        else:\n",
    "            # Fallback: If no face found, use the center-crop of the frame\n",
    "            # This ensures we *always* get 30 frames\n",
    "            h, w, _ = frame.shape\n",
    "            cx, cy = w // 2, h // 2\n",
    "            crop_size = min(h, w)\n",
    "            face_crop = frame[cy - crop_size // 2:cy + crop_size // 2, cx - crop_size // 2:cx + crop_size // 2]\n",
    "\n",
    "        # --- Transform ---\n",
    "        # Convert BGR (OpenCV) to RGB (PIL)\n",
    "        face_crop_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(face_crop_rgb)\n",
    "        \n",
    "        # Apply the same transforms as training\n",
    "        transformed_frame = data_transform(pil_image)\n",
    "        frame_sequence.append(transformed_frame)\n",
    "        frames_processed_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if frames_processed_count != NUM_FRAMES:\n",
    "        print(f\"Warning: Could only process {frames_processed_count} of {NUM_FRAMES} frames.\")\n",
    "        if frames_processed_count == 0:\n",
    "            return None\n",
    "        while len(frame_sequence) < NUM_FRAMES:\n",
    "            frame_sequence.append(frame_sequence[-1])\n",
    "            \n",
    "    # Stack the 30 frames into a single tensor\n",
    "    sequence_tensor = torch.stack(frame_sequence)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    return sequence_tensor.unsqueeze(0)\n",
    "\n",
    "# --- 6. Main Prediction Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if not os.path.exists(SAVED_MODEL_PATH):\n",
    "        print(f\"Error: Model file not found at '{SAVED_MODEL_PATH}'\")\n",
    "        print(\"Please make sure the trained 'deepfake_temporal_detector.pth' is in the same directory.\")\n",
    "        exit()\n",
    "    if face_cascade.empty():\n",
    "        print(f\"Error: Could not load Haar Cascade file for face detection.\")\n",
    "        exit()\n",
    "\n",
    "    # --- Load Model ---\n",
    "    print(\"Loading temporal (EfficientNet+LSTM) model...\")\n",
    "    model = CNN_LSTM_Model().to(device)\n",
    "    model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n",
    "    model.eval() # Set model to evaluation mode (CRITICAL)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # --- Prediction Loop ---\n",
    "    while True:\n",
    "        video_path = input(\"\\nEnter the path to the video you want to analyze (or 'exit' to quit): \").strip()\n",
    "        \n",
    "        if video_path.lower() == 'exit':\n",
    "            break\n",
    "            \n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Error: The file was not found at '{video_path}'. Please check the path.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nAnalyzing video: {video_path}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 1. Process the video into a frame sequence tensor\n",
    "        input_tensor = process_video_for_prediction(video_path)\n",
    "        \n",
    "        if input_tensor is None:\n",
    "            print(\"Could not process video.\")\n",
    "            continue\n",
    "            \n",
    "        # 2. Send tensor to the GPU\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        # 3. Get prediction\n",
    "        with torch.no_grad(): # Disable gradient calculation for inference\n",
    "            logit = model(input_tensor)\n",
    "            prob = torch.sigmoid(logit).item() # Get final probability\n",
    "            \n",
    "        end_time = time.time()\n",
    "\n",
    "        # 4. Report the verdict\n",
    "        if prob < 0.5:\n",
    "            confidence = (1 - prob) * 100\n",
    "            print(f\"\\n--- Verdict: FAKE ---\")\n",
    "            print(f\"Confidence: {confidence:.2f}%\")\n",
    "        else:\n",
    "            confidence = prob * 100\n",
    "            print(f\"\\n--- Verdict: REAL ---\")\n",
    "            print(f\"Confidence: {confidence:.2f}%\")\n",
    "            \n",
    "        print(f\"Analysis took {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbdd1d-fca6-4fae-a5be-4f6828d74bff",
   "metadata": {},
   "source": [
    "stack training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c9ef00-906d-488c-a1d1-bceba84ba104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M2 GPU (mps).\n",
      "\n",
      "--- Loading Ensemble Models ---\n",
      "Loading ResNet50...\n",
      "Loading EfficientNet_B0...\n",
      "\n",
      "Models loaded and set to eval() mode.\n",
      "\n",
      "--- Phase 1: Starting Feature Generation ---\n",
      "\n",
      "Processing 998 REAL videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Real Videos:  76%|███████████████████▊      | 760/998 [3:06:22<59:51, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No faces found in /Users/visheshbishnoi/Desktop/videos/real/celeb_real_0475.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Real Videos: 100%|██████████████████████████| 998/998 [4:08:58<00:00, 14.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 1000 FAKE videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fake Videos:  51%|███████████▋           | 506/1000 [1:55:40<1:28:35, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No faces found in /Users/visheshbishnoi/Desktop/videos/fake/dfdc_fake_0931.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fake Videos: 100%|████████████████████████| 1000/1000 [3:46:33<00:00, 13.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation complete. Generated 1996 feature vectors.\n",
      "\n",
      "--- Phase 2: Training Stacking Meta-Model ---\n",
      "Training Logistic Regression model...\n",
      "\n",
      "Meta-Model Test Accuracy: 0.9200\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    FAKE (0)       0.97      0.87      0.92       200\n",
      "    REAL (1)       0.88      0.97      0.92       200\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.92      0.92      0.92       400\n",
      "weighted avg       0.92      0.92      0.92       400\n",
      "\n",
      "\n",
      "Training final model on ALL data...\n",
      "Successfully trained and saved meta-model to: stacking_logistic_regression.pkl\n",
      "Successfully saved feature scaler to: stacking_scaler.pkl\n",
      "\n",
      "Training pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "# train_stacking_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib # For saving the model\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# UPDATE THESE PATHS\n",
    "MODEL_PATHS = {\n",
    "    \"ResNet50\": 'deepfake_detector_ResNet50.pth',\n",
    "    \"EfficientNet_B0\": 'deepfake_detector_EfficientNet_B0.pth'\n",
    "}\n",
    "HAAR_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "# UPDATE to your video dataset\n",
    "REAL_VIDEO_DIR = \"/Users/visheshbishnoi/Desktop/videos/real\"\n",
    "FAKE_VIDEO_DIR = \"/Users/visheshbishnoi/Desktop/videos/fake\"\n",
    "META_MODEL_SAVE_PATH = \"stacking_logistic_regression.pkl\"\n",
    "SCALER_SAVE_PATH = \"stacking_scaler.pkl\" # IMPORTANT for new data\n",
    "\n",
    "# --- 2. Device & Model Setup (Copied from your script) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M2 GPU (mps).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def create_resnet50_model():\n",
    "    model = models.resnet50(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 1))\n",
    "    return model\n",
    "\n",
    "def create_efficientnet_model():\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=True), nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(512, 1))\n",
    "    return model\n",
    "\n",
    "def load_all_models(paths):\n",
    "    models_ensemble = {}\n",
    "    print(\"\\n--- Loading Ensemble Models ---\")\n",
    "    \n",
    "    # Load ResNet50\n",
    "    if os.path.exists(paths[\"ResNet50\"]):\n",
    "        print(f\"Loading ResNet50...\")\n",
    "        model_resnet = create_resnet50_model()\n",
    "        model_resnet.load_state_dict(torch.load(paths[\"ResNet50\"], map_location=device))\n",
    "        model_resnet = model_resnet.to(device)\n",
    "        model_resnet.eval()\n",
    "        models_ensemble[\"ResNet50\"] = model_resnet\n",
    "    else:\n",
    "        print(f\"FATAL: ResNet50 model not found at '{paths['ResNet50']}'.\")\n",
    "        return None\n",
    "\n",
    "    # Load EfficientNet_B0\n",
    "    if os.path.exists(paths[\"EfficientNet_B0\"]):\n",
    "        print(f\"Loading EfficientNet_B0...\")\n",
    "        model_efficientnet = create_efficientnet_model()\n",
    "        model_efficientnet.load_state_dict(torch.load(paths[\"EfficientNet_B0\"], map_location=device))\n",
    "        model_efficientnet = model_efficientnet.to(device)\n",
    "        model_efficientnet.eval()\n",
    "        models_ensemble[\"EfficientNet_B0\"] = model_efficientnet\n",
    "    else:\n",
    "        print(f\"FATAL: EfficientNet_B0 model not found at '{paths['EfficientNet_B0']}'.\")\n",
    "        return None\n",
    "        \n",
    "    print(\"\\nModels loaded and set to eval() mode.\")\n",
    "    return models_ensemble\n",
    "\n",
    "# --- 3. Phase 1: Feature Extraction Function ---\n",
    "def get_features_for_video(video_path, models_ensemble, face_cascade):\n",
    "    \"\"\"\n",
    "    Processes a single video and returns a feature vector based on predictions.\n",
    "    Returns None if no faces are found.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video file {video_path}\")\n",
    "        return None\n",
    "\n",
    "    # Store all frame-level probabilities for each model\n",
    "    model_probs = {name: [] for name in models_ensemble.keys()}\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            (x, y, w, h) = faces[0]\n",
    "            face_crop_bgr = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            face_crop_rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(face_crop_rgb)\n",
    "            input_tensor = data_transform(pil_image)\n",
    "            input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for name, model in models_ensemble.items():\n",
    "                    logit = model(input_batch)\n",
    "                    prob = torch.sigmoid(logit).item()\n",
    "                    model_probs[name].append(prob)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    # --- Feature Calculation ---\n",
    "    # Check if any faces were found at all\n",
    "    if not any(model_probs.values()):\n",
    "        print(f\"Warning: No faces found in {video_path}\")\n",
    "        return None\n",
    "\n",
    "    features = []\n",
    "    for name in models_ensemble.keys():\n",
    "        probs = model_probs[name]\n",
    "        if not probs:\n",
    "            # Handle case where one model ran but no faces were found (should be caught above, but as a safeguard)\n",
    "            probs = [0.5] # Neutral value if no faces detected\n",
    "            \n",
    "        features.append(np.mean(probs))\n",
    "        features.append(np.std(probs))\n",
    "        features.append(np.median(probs))\n",
    "        \n",
    "    # Our feature vector is [resnet_avg, resnet_std, resnet_median, effnet_avg, effnet_std, effnet_median]\n",
    "    return features\n",
    "\n",
    "# --- 4. Main Execution (Phase 1 & 2) ---\n",
    "if __name__ == '__main__':\n",
    "    models = load_all_models(MODEL_PATHS)\n",
    "    face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "    \n",
    "    if models is None:\n",
    "        print(\"Exiting. Base models could not be loaded.\")\n",
    "        exit()\n",
    "\n",
    "    X_meta = [] # To store our feature vectors\n",
    "    y_meta = [] # To store our labels (0=fake, 1=real)\n",
    "\n",
    "    # --- Phase 1: Process all videos ---\n",
    "    print(\"\\n--- Phase 1: Starting Feature Generation ---\")\n",
    "    \n",
    "    # Process REAL videos (Label = 1)\n",
    "    real_files = [os.path.join(REAL_VIDEO_DIR, f) for f in os.listdir(REAL_VIDEO_DIR) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    print(f\"\\nProcessing {len(real_files)} REAL videos...\")\n",
    "    for video_path in tqdm(real_files, desc=\"Real Videos\"):\n",
    "        features = get_features_for_video(video_path, models, face_cascade)\n",
    "        if features:\n",
    "            X_meta.append(features)\n",
    "            y_meta.append(1)\n",
    "\n",
    "    # Process FAKE videos (Label = 0)\n",
    "    fake_files = [os.path.join(FAKE_VIDEO_DIR, f) for f in os.listdir(FAKE_VIDEO_DIR) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    print(f\"\\nProcessing {len(fake_files)} FAKE videos...\")\n",
    "    for video_path in tqdm(fake_files, desc=\"Fake Videos\"):\n",
    "        features = get_features_for_video(video_path, models, face_cascade)\n",
    "        if features:\n",
    "            X_meta.append(features)\n",
    "            y_meta.append(0)\n",
    "\n",
    "    print(f\"\\nFeature generation complete. Generated {len(X_meta)} feature vectors.\")\n",
    "\n",
    "    # --- Phase 2: Train the Meta-Model ---\n",
    "    print(\"\\n--- Phase 2: Training Stacking Meta-Model ---\")\n",
    "    \n",
    "    X = np.array(X_meta)\n",
    "    y = np.array(y_meta)\n",
    "\n",
    "    # Split the data for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # CRITICAL: Scale the features\n",
    "    # Tree-based models don't need this, but Logistic Regression and SVMs do.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    # You can add class_weight='balanced' if your dataset is imbalanced\n",
    "    meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    meta_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate the meta-model\n",
    "    y_pred = meta_model.predict(X_test_scaled)\n",
    "    print(f\"\\nMeta-Model Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"FAKE (0)\", \"REAL (1)\"]))\n",
    "\n",
    "    # --- Final Step: Train on ALL data and Save ---\n",
    "    print(\"\\nTraining final model on ALL data...\")\n",
    "    # We re-fit the scaler and model on 100% of the data for deployment\n",
    "    final_scaler = StandardScaler()\n",
    "    X_scaled_full = final_scaler.fit_transform(X)\n",
    "    \n",
    "    final_meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    final_meta_model.fit(X_scaled_full, y)\n",
    "    \n",
    "    # Save the scaler and the model\n",
    "    joblib.dump(final_meta_model, META_MODEL_SAVE_PATH)\n",
    "    joblib.dump(final_scaler, SCALER_SAVE_PATH)\n",
    "    \n",
    "    print(f\"Successfully trained and saved meta-model to: {META_MODEL_SAVE_PATH}\")\n",
    "    print(f\"Successfully saved feature scaler to: {SCALER_SAVE_PATH}\")\n",
    "    print(\"\\nTraining pipeline complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e1621-2690-4b11-9b05-be56e54f55dd",
   "metadata": {},
   "source": [
    "stack predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271e8412-e493-4e1e-9aab-408fccab1f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple M2 GPU (mps).\n",
      "\n",
      "--- Loading Ensemble Models ---\n",
      "Loading ResNet50...\n",
      "Loading EfficientNet_B0...\n",
      "Base models loaded.\n",
      "Loading meta-model from stacking_logistic_regression.pkl...\n",
      "Loading scaler from stacking_scaler.pkl...\n",
      "Stacking model and scaler loaded.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_0046.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file was not found at '/Users/visheshbishnoi/Desktop/data/fake/dfdc_fake_0046.mp4'. Please check the path.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/videos/fake/dfdc_fake_1351.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/videos/fake/dfdc_fake_1351.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 299/299 [00:10<00:00, 27.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 10.80 seconds\n",
      "Frames with faces detected: 219\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL\n",
      "Model Confidence: 89.22%\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):    /Users/visheshbishnoi/Desktop/videos/fake/dfdc_fake_1366.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/videos/fake/dfdc_fake_1366.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 299/299 [00:12<00:00, 24.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 12.26 seconds\n",
      "Frames with faces detected: 294\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY REAL\n",
      "Model Confidence: 89.89%\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  /Users/visheshbishnoi/Desktop/videos/fake/celeb_fake_4969.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing video file: /Users/visheshbishnoi/Desktop/videos/fake/celeb_fake_4969.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Video: 100%|███████████████████████| 441/441 [00:14<00:00, 31.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Analysis Complete ---\n",
      "Time taken: 14.14 seconds\n",
      "Frames with faces detected: 435\n",
      "---------------------------------\n",
      "Final Verdict: LIKELY FAKE\n",
      "Model Confidence: 99.95%\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the path to the video you want to analyze (or 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# predict_with_stacking.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import joblib # <-- Import joblib\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "MODEL_PATHS = {\n",
    "    \"ResNet50\": 'deepfake_detector_ResNet50.pth',\n",
    "    \"EfficientNet_B0\": 'deepfake_detector_EfficientNet_B0.pth'\n",
    "}\n",
    "HAAR_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "# --- NEW: Paths to the saved stacking model ---\n",
    "META_MODEL_PATH = \"stacking_logistic_regression.pkl\"\n",
    "SCALER_PATH = \"stacking_scaler.pkl\"\n",
    "\n",
    "# --- 2. Device Setup ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M2 GPU (mps).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# --- 3. Data Transform ---\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 4. Model Creation Functions (Same as before) ---\n",
    "def create_resnet50_model():\n",
    "    model = models.resnet50(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 1))\n",
    "    return model\n",
    "\n",
    "def create_efficientnet_model():\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(nn.Dropout(p=0.2, inplace=True), nn.Linear(num_ftrs, 512), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(512, 1))\n",
    "    return model\n",
    "\n",
    "# --- 5. Load All Models (Base + Stacking) ---\n",
    "def load_all_models_and_stacker(paths, stacker_path, scaler_path):\n",
    "    # Load base models\n",
    "    models_ensemble = {}\n",
    "    print(\"\\n--- Loading Ensemble Models ---\")\n",
    "    \n",
    "    if os.path.exists(paths[\"ResNet50\"]):\n",
    "        print(f\"Loading ResNet50...\")\n",
    "        model_resnet = create_resnet50_model()\n",
    "        model_resnet.load_state_dict(torch.load(paths[\"ResNet50\"], map_location=device))\n",
    "        model_resnet = model_resnet.to(device)\n",
    "        model_resnet.eval()\n",
    "        models_ensemble[\"ResNet50\"] = model_resnet\n",
    "    else:\n",
    "        print(f\"Error: Model file not found at '{paths['ResNet50']}'.\")\n",
    "        return None, None, None\n",
    "\n",
    "    if os.path.exists(paths[\"EfficientNet_B0\"]):\n",
    "        print(f\"Loading EfficientNet_B0...\")\n",
    "        model_efficientnet = create_efficientnet_model()\n",
    "        model_efficientnet.load_state_dict(torch.load(paths[\"EfficientNet_B0\"], map_location=device))\n",
    "        model_efficientnet = model_efficientnet.to(device)\n",
    "        model_efficientnet.eval()\n",
    "        models_ensemble[\"EfficientNet_B0\"] = model_efficientnet\n",
    "    else:\n",
    "        print(f\"Error: Model file not found at '{paths['EfficientNet_B0']}'.\")\n",
    "        return None, None, None\n",
    "        \n",
    "    print(\"Base models loaded.\")\n",
    "\n",
    "    # Load stacking meta-model and scaler\n",
    "    try:\n",
    "        print(f\"Loading meta-model from {stacker_path}...\")\n",
    "        meta_model = joblib.load(stacker_path)\n",
    "        print(f\"Loading scaler from {scaler_path}...\")\n",
    "        scaler = joblib.load(scaler_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Could not load stacking model or scaler. {e}\")\n",
    "        print(\"Please run 'train_stacking_model.py' first.\")\n",
    "        return None, None, None\n",
    "        \n",
    "    print(\"Stacking model and scaler loaded.\")\n",
    "    return models_ensemble, meta_model, scaler\n",
    "\n",
    "# --- 6. MODIFIED Video Analysis Function ---\n",
    "def analyze_video_with_stacking(video_path, models_ensemble, face_cascade, meta_model, scaler):\n",
    "    \"\"\"\n",
    "    Analyzes a video, generates features, and uses the stacking model\n",
    "    for a final prediction.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file at '{video_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    except:\n",
    "        total_frames = 0\n",
    "        \n",
    "    print(f\"\\nAnalyzing video file: {video_path}\")\n",
    "    \n",
    "    # Store all frame-level probabilities\n",
    "    model_probs = {name: [] for name in models_ensemble.keys()}\n",
    "    faces_found_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            faces_found_count += 1\n",
    "            (x, y, w, h) = faces[0]\n",
    "            \n",
    "            face_crop_bgr = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            face_crop_rgb = cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(face_crop_rgb)\n",
    "            input_tensor = data_transform(pil_image)\n",
    "            input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for name, model in models_ensemble.items():\n",
    "                    logit = model(input_batch)\n",
    "                    prob = torch.sigmoid(logit).item()\n",
    "                    model_probs[name].append(prob)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    cap.release()\n",
    "    \n",
    "    print(\"\\n--- Video Analysis Complete ---\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Frames with faces detected: {faces_found_count}\")\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "    if faces_found_count == 0:\n",
    "        print(\"Final Verdict: UNKNOWN (No faces were detected in the video)\")\n",
    "        print(\"---------------------------------\")\n",
    "        return\n",
    "\n",
    "    # --- Generate Feature Vector (MUST match training script) ---\n",
    "    features = []\n",
    "    for name in models_ensemble.keys():\n",
    "        probs = model_probs[name]\n",
    "        if not probs:\n",
    "            probs = [0.5] # Handle no-face edge case\n",
    "            \n",
    "        features.append(np.mean(probs))\n",
    "        features.append(np.std(probs))\n",
    "        features.append(np.median(probs))\n",
    "        \n",
    "    # Reshape for a single prediction\n",
    "    features_np = np.array(features).reshape(1, -1)\n",
    "    \n",
    "    # --- Scale the features ---\n",
    "    features_scaled = scaler.transform(features_np)\n",
    "    \n",
    "    # --- Get Final Prediction ---\n",
    "    prediction = meta_model.predict(features_scaled)[0]\n",
    "    probability = meta_model.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    if prediction == 0:\n",
    "        verdict = \"FAKE\"\n",
    "        confidence = probability[0] # Probability of class 0\n",
    "    else:\n",
    "        verdict = \"REAL\"\n",
    "        confidence = probability[1] # Probability of class 1\n",
    "\n",
    "    print(f\"Final Verdict: LIKELY {verdict}\")\n",
    "    print(f\"Model Confidence: {confidence*100:.2f}%\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "# --- 7. Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    base_models, meta_model, scaler = load_all_models_and_stacker(MODEL_PATHS, META_MODEL_PATH, SCALER_PATH)\n",
    "    face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "    \n",
    "    if base_models and meta_model and scaler: # Only proceed if all models loaded\n",
    "        while True:\n",
    "            video_path = input(\"\\nEnter the path to the video you want to analyze (or 'exit' to quit): \").strip()\n",
    "            \n",
    "            if video_path.lower() == 'exit':\n",
    "                break\n",
    "                \n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"Error: The file was not found at '{video_path}'. Please check the path.\")\n",
    "                continue\n",
    "                \n",
    "            analyze_video_with_stacking(video_path, base_models, face_cascade, meta_model, scaler)\n",
    "    else:\n",
    "        print(\"Exiting. Not all required models were loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00e1f5-30dd-4b6d-ab02-d21ec66bc8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
